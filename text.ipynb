{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f899a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import contractions\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d74ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf164bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rudyferreira/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rudyferreira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842978a4",
   "metadata": {},
   "source": [
    "### initial data treatment and visualization\n",
    "transforming into pandas dataframe, removing upper case letters and punctuation, visualizing distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605e1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\")\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88be83fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('label').quote.count().plot.bar(color = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43feeb6e",
   "metadata": {},
   "source": [
    "### data processing\n",
    "removing stopwords, uppercase letters, lemmatizing and applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1282fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # Keep important words for climate change context\n",
    "        self.domain_specific_words = {\n",
    "            'climate', 'change', 'global', 'warming', 'carbon', 'emission',\n",
    "            'temperature', 'science', 'scientific', 'study', 'research',\n",
    "            'evidence', 'data', 'fossil', 'fuel', 'energy', 'renewable',\n",
    "            'environment', 'environmental', 'pollution', 'co2', 'greenhouse', 'hoax', 'false',\n",
    "            'lie', 'government', 'scientist', 'scientists'\n",
    "        }\n",
    "        self.stop_words = self.stop_words - self.domain_specific_words - {'no', 'not', 'nor', 'none', 'never', 'nothing', \"my\",\"haven't\",\"aren't\",\"can\",\"no\", \"why\", \"through\", \"herself\", \"she\", \"he\", \"himself\", \"you\", \"you're\", \"myself\", \"not\", \"here\", \"some\", \"do\", \"does\", \"did\", \"will\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"should\", \"should've\", \"couldn't\", \"mightn't\", \"mustn't\", \"shouldn't\", \"hadn't\", \"wasn't\", \"wouldn't\"}\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        # Expand contractions\n",
    "        text = contractions.fix(text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters but keep important punctuation and symbols\n",
    "        text = re.sub(r'[^\\w\\s!?.,Â°]', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        return ' '.join([self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                        if token not in self.stop_words or token in self.domain_specific_words])\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.lemmatize_text(self.clean_text(text)) for text in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa92d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.w2v_model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        tokenized_corpus = [sentence.split() for sentence in X]\n",
    "        self.w2v_model = Word2Vec(\n",
    "            sentences=tokenized_corpus,\n",
    "            vector_size=self.vector_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=4,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        tokenized_corpus = [sentence.split() for sentence in X]\n",
    "        document_vectors = [\n",
    "            self.document_vector(doc) for doc in tokenized_corpus\n",
    "        ]\n",
    "        return np.array(document_vectors)\n",
    "\n",
    "    def document_vector(self, doc):\n",
    "        words = [word for word in doc if word in self.w2v_model.wv.key_to_index]\n",
    "        if len(words) == 0:\n",
    "            return np.zeros(self.vector_size)\n",
    "        return np.mean(self.w2v_model.wv[words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c94bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_LR(X, y):\n",
    "    \"\"\"Train the model and print evaluation metrics\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    pipeline_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', LogisticRegression(max_iter = 1000, verbose = True))\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'tfidf__max_features': [5000, 10000, None],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
    "        'tfidf__min_df': [1, 3, 5],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__solver': ['lbfgs', 'liblinear'],\n",
    "    }\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline_lr,\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1_macro',\n",
    "        verbose=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nBest Parameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(\"\\nBest F1-macro Score: {:.3f}\".format(grid_search.best_score_))\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    cv_scores = cross_val_score(pipeline_lr, X, y, cv=cv, scoring='f1_macro')\n",
    "    print(\"\\nCross-validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Average F1-macro score: {:.3f} (+/- {:.3f})\".format(\n",
    "        cv_scores.mean(), cv_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f544f3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LibLinear]\n",
      "Best Parameters:\n",
      "{'classifier__C': 1, 'classifier__solver': 'liblinear', 'tfidf__max_features': None, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "\n",
      "Best F1-macro Score: 0.626\n",
      "\n",
      "Classification Report:\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                 0_not_relevant       0.71      0.63      0.67       243\n",
      "                1_not_happening       0.71      0.74      0.72       111\n",
      "                    2_not_human       0.64      0.73      0.68       105\n",
      "                      3_not_bad       0.66      0.53      0.59        58\n",
      "4_solutions_harmful_unnecessary       0.61      0.54      0.58       116\n",
      "           5_science_unreliable       0.58      0.57      0.57       120\n",
      "            6_proponents_biased       0.56      0.62      0.59       118\n",
      "          7_fossil_fuels_needed       0.45      0.65      0.53        43\n",
      "\n",
      "                       accuracy                           0.63       914\n",
      "                      macro avg       0.61      0.63      0.62       914\n",
      "                   weighted avg       0.64      0.63      0.63       914\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[153   9   7   6  20  16  21  11]\n",
      " [  3  82   9   2   1   8   6   0]\n",
      " [  5   7  77   2   4   8   2   0]\n",
      " [  1   5  10  31   3   2   5   1]\n",
      " [ 13   2   2   1  63   4  10  21]\n",
      " [ 14   6  15   3   2  68  12   0]\n",
      " [ 23   5   0   1   4  11  73   1]\n",
      " [  5   0   0   1   6   1   2  28]]\n",
      "\n",
      "Cross-validation F1-macro scores: [0.6183337  0.64859152 0.64526078 0.59246513 0.63059365]\n",
      "Average F1-macro score: 0.627 (+/- 0.041)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X = df['quote']\n",
    "    y = df['label']\n",
    "    \n",
    "    pipeline = train_and_evaluate_LR(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7332a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fa999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(X, y, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(\n",
    "        X,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return encodings, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9cf74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_BERT(X, y):\n",
    "    \"\"\"Train the BERT model and print evaluation metrics\"\"\"\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Tokenize text and obtain embeddings for SMOTE\n",
    "    train_encodings = tokenizer(\n",
    "        X_train.tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    X_train_embeddings = train_encodings[\"input_ids\"].numpy()\n",
    "    \n",
    "    # Apply SMOTE on embeddings\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_embeddings, y_train)\n",
    "\n",
    "    # Convert back to tokenized format\n",
    "    X_train_resampled = torch.tensor(X_train_resampled)\n",
    "    y_train_resampled = torch.tensor(y_train_resampled)\n",
    "\n",
    "    # Tokenize the test set\n",
    "    test_encodings, y_test = preprocess_data(X_test.tolist(), y_test, tokenizer)\n",
    "\n",
    "    # Create Dataset objects\n",
    "    train_dataset = TextDataset(\n",
    "        {\"input_ids\": X_train_resampled}, y_train_resampled.tolist()\n",
    "    )\n",
    "    test_dataset = TextDataset(test_encodings, y_test)\n",
    "\n",
    "    # Load pre-trained BERT model\n",
    "    num_classes = len(np.unique(y))\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=num_classes\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=lambda p: {\n",
    "            \"f1_macro\": f1_score(\n",
    "                p.label_ids, np.argmax(p.predictions, axis=1), average=\"macro\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"\\nEvaluation Results: {eval_results}\")\n",
    "\n",
    "    # Test set predictions\n",
    "    y_pred_logits = trainer.predict(test_dataset).predictions\n",
    "    y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Preprocess fold data\n",
    "        train_encodings, y_train_fold = preprocess_data(\n",
    "            X_train_fold, y_train_fold, tokenizer\n",
    "        )\n",
    "        val_encodings, y_val_fold = preprocess_data(X_val_fold, y_val_fold, tokenizer)\n",
    "\n",
    "        train_dataset = TextDataset(train_encodings, y_train_fold)\n",
    "        val_dataset = TextDataset(val_encodings, y_val_fold)\n",
    "\n",
    "        # Train and evaluate on the fold\n",
    "        trainer.train_dataset = train_dataset\n",
    "        trainer.eval_dataset = val_dataset\n",
    "        trainer.train()\n",
    "        val_results = trainer.evaluate()\n",
    "        cv_scores.append(val_results[\"eval_f1_macro\"])\n",
    "\n",
    "    print(\"\\nCross-validation F1-macro scores:\", cv_scores)\n",
    "    print(\"Average F1-macro score: {:.3f} (+/- {:.3f})\".format(\n",
    "        np.mean(cv_scores), np.std(cv_scores) * 2\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c701132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/rudyferreira/opt/anaconda3/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='317' max='1941' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 317/1941 58:28 < 5:01:29, 0.09 it/s, Epoch 0.49/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bae1df4ed3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_BERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-36d990989d75>\u001b[0m in \u001b[0;36mtrain_and_evaluate_BERT\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2479\u001b[0m                     )\n\u001b[1;32m   2480\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                     if (\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3578\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3579\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3581\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3634\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1669\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 515\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    438\u001b[0m         )\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X = df['quote']\n",
    "    y = df['label']\n",
    "    \n",
    "    pipeline = train_and_evaluate_BERT(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
